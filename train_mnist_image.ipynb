{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "mnist=tf.keras.datasets.mnist\n",
    "k = mnist.load_data()\n",
    "((train_mnist_image, train_mnist_label), _) = k\n",
    "train_mnist_image = train_mnist_image.reshape(train_mnist_image.shape[0], 28, 28, 1).astype('float32')\n",
    "train_mnist_image = (train_mnist_image - 127.5)/127.5\n",
    "print(train_mnist_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_image(arr):\n",
    "    two_d = (np.reshape(arr, (28, 28)))\n",
    "    plt.imshow(two_d, interpolation='nearest') #cmap=plt.get_cmap('gray')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, batch_size = 128):\n",
    "    input_shape = X.shape[0]\n",
    "    image_list = np.array(range(input_shape))\n",
    "    np.random.shuffle(image_list)\n",
    "    iterations = int(input_shape / batch_size)\n",
    "    image = []\n",
    "    for i in range(iterations-1):\n",
    "        batch_list = image_list[i * (batch_size) : (i+1) * (batch_size)]\n",
    "        image.append(X[batch_list])\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    X_gen_input = Input(shape=(10))\n",
    "    X = Dense(256, activation='relu')(X_gen_input)\n",
    "    X = Dropout(.5)(X)\n",
    "    X = Dense(144, activation='relu')(X)\n",
    "    X = Reshape((12, 12, 1))(X)\n",
    "    \n",
    "    X = Conv2DTranspose(16, (3, 3), strides=(1, 1), padding='valid', activation='relu')(X)\n",
    "    X = UpSampling2D(size=(2, 2), interpolation='nearest')(X)\n",
    "    \n",
    "    X = Conv2DTranspose(8, (3, 3), strides=(1, 1), padding='same', activation='relu')(X)\n",
    "    X = Dropout(.5)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(4, (3, 3), strides=(1, 1), padding='same', activation='relu')(X)\n",
    "    X = Dropout(.5)(X)\n",
    "    \n",
    "    X = Conv2DTranspose(2, (3, 3), strides=(1, 1), padding='same', activation='relu')(X)\n",
    "    X = Dropout(.5)(X)\n",
    "    \n",
    "    output = Conv2DTranspose(1, (3, 3), strides=(1, 1), padding='same', activation='tanh')(X)\n",
    "    \n",
    "    model = Model(inputs = X_gen_input, outputs = output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    X_dis_input = Input(shape=(28, 28, 1))\n",
    "    X = Conv2D(2, (3, 3), strides=(1, 1), padding='same', activation='relu')(X_dis_input)\n",
    "    X = Conv2D(4, (3, 3), strides=(1, 1), padding='same', activation='relu')(X)\n",
    "    X = Conv2D(8, (3, 3), strides=(1, 1), padding='same', activation='relu')(X)\n",
    "    X = MaxPooling2D((2, 2), strides=(1, 1))(X)\n",
    "    X = Conv2D(16, (3, 3), strides=(1, 1), padding='valid', activation='relu')(X)\n",
    "    X = Flatten()(X)\n",
    "                                                                             \n",
    "    X = Dense(256, activation='relu')(X)\n",
    "    X = Dropout(.5)(X)\n",
    "    X = Dense(128, activation = 'relu')(X)\n",
    "    X = Dropout(.5)(X)\n",
    "    output_dis = Dense(1, activation = 'sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs = X_dis_input, outputs = output_dis)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_model()\n",
    "discriminator = discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(train_mnist_images):\n",
    "        shape = train_mnist_images.shape\n",
    "#         print(train_mnist_images.shape)\n",
    "#         print(shape[0])\n",
    "        noise_batch = np.random.normal(0, 1, (shape[0], 10))\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            \n",
    "            fake_image_batch = generator(noise_batch, training=True)\n",
    "            real_output = discriminator(train_mnist_images, training=True)\n",
    "            fake_output = discriminator(fake_image_batch, training=True)\n",
    "            \n",
    "            real_disc_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "            fake_disc_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "            \n",
    "            gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "            discriminator_loss  = (real_disc_loss + fake_disc_loss) \n",
    "            \n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "        \n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "        return [gen_loss, discriminator_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAADnCAYAAACEyTRLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADcUlEQVR4nO3bsW3EQAwAwaWh/lumO1AmnCXPpJcQDBYE/jW7G8B/93N6AIC/QAwBEkOASgwBKjEEqOq6e5yZV//UvLtzeoY79vscu33OV3frMgRIDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKhqdvf0DADHuQwBEkOASgwBKjEEqMQQoBJDgEoMASoxBKjqunucmVf/I3t35/QMd+z3OXb7nK/u1mUIkBgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUNbt7egaA41yGAIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQ1XX3ODOv/jxld+f0DHfs9zl2+5yv7tZlCJAYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVDW7e3oGgONchgCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUNV19zgzr/48ZXfn9Ax37Pc5dvucr+7WZQiQGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlQ1u3t6BoDjXIYAiSFAJYYAlRgCVGIIUIkhQFW/DzpQwzEfSLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_discriminator_loss: tf.Tensor(0.00047565106, shape=(), dtype=float32)\n",
      "epoch: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAADnCAYAAACEyTRLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADcUlEQVR4nO3bsW3EQAwAwaWh/lumO1AmnCXPpJcQDBYE/jW7G8B/93N6AIC/QAwBEkOASgwBKjEEqOq6e5yZV//UvLtzeoY79vscu33OV3frMgRIDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKhqdvf0DADHuQwBEkOASgwBKjEEqMQQoBJDgEoMASoxBKjqunucmVf/I3t35/QMd+z3OXb7nK/u1mUIkBgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUNbt7egaA41yGAIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQ1XX3ODOv/jxld+f0DHfs9zl2+5yv7tZlCJAYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVDW7e3oGgONchgCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUNV19zgzr/48ZXfn9Ax37Pc5dvucr+7WZQiQGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlQ1u3t6BoDjXIYAiSFAJYYAlRgCVGIIUIkhQFW/DzpQwzEfSLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_discriminator_loss: tf.Tensor(5.6276727e-05, shape=(), dtype=float32)\n",
      "epoch: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAADnCAYAAACEyTRLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADcUlEQVR4nO3bsW3EQAwAwaWh/lumO1AmnCXPpJcQDBYE/jW7G8B/93N6AIC/QAwBEkOASgwBKjEEqOq6e5yZV//UvLtzeoY79vscu33OV3frMgRIDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKhqdvf0DADHuQwBEkOASgwBKjEEqMQQoBJDgEoMASoxBKjqunucmVf/I3t35/QMd+z3OXb7nK/u1mUIkBgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUNbt7egaA41yGAIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQ1XX3ODOv/jxld+f0DHfs9zl2+5yv7tZlCJAYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVDW7e3oGgONchgCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUNV19zgzr/48ZXfn9Ax37Pc5dvucr+7WZQiQGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlQ1u3t6BoDjXIYAiSFAJYYAlRgCVGIIUIkhQFW/DzpQwzEfSLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_discriminator_loss: tf.Tensor(1.1924397e-05, shape=(), dtype=float32)\n",
      "epoch: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAADnCAYAAACEyTRLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADcUlEQVR4nO3bsW3EQAwAwaWh/lumO1AmnCXPpJcQDBYE/jW7G8B/93N6AIC/QAwBEkOASgwBKjEEqOq6e5yZV//UvLtzeoY79vscu33OV3frMgRIDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKhqdvf0DADHuQwBEkOASgwBKjEEqMQQoBJDgEoMASoxBKjqunucmVf/I3t35/QMd+z3OXb7nK/u1mUIkBgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUNbt7egaA41yGAIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQ1XX3ODOv/jxld+f0DHfs9zl2+5yv7tZlCJAYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVDW7e3oGgONchgCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUNV19zgzr/48ZXfn9Ax37Pc5dvucr+7WZQiQGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlQ1u3t6BoDjXIYAiSFAJYYAlRgCVGIIUIkhQFW/DzpQwzEfSLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_discriminator_loss: tf.Tensor(5.592393e-06, shape=(), dtype=float32)\n",
      "epoch: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAADnCAYAAACEyTRLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADcUlEQVR4nO3bsW3EQAwAwaWh/lumO1AmnCXPpJcQDBYE/jW7G8B/93N6AIC/QAwBEkOASgwBKjEEqOq6e5yZV//UvLtzeoY79vscu33OV3frMgRIDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKhqdvf0DADHuQwBEkOASgwBKjEEqMQQoBJDgEoMASoxBKjqunucmVf/I3t35/QMd+z3OXb7nK/u1mUIkBgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUNbt7egaA41yGAIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQ1XX3ODOv/jxld+f0DHfs9zl2+5yv7tZlCJAYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVDW7e3oGgONchgCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUNV19zgzr/48ZXfn9Ax37Pc5dvucr+7WZQiQGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlQ1u3t6BoDjXIYAiSFAJYYAlRgCVGIIUIkhQFW/DzpQwzEfSLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_discriminator_loss: tf.Tensor(2.5833679e-06, shape=(), dtype=float32)\n",
      "epoch: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAADnCAYAAACEyTRLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADcUlEQVR4nO3bsW3EQAwAwaWh/lumO1AmnCXPpJcQDBYE/jW7G8B/93N6AIC/QAwBEkOASgwBKjEEqOq6e5yZV//UvLtzeoY79vscu33OV3frMgRIDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKhqdvf0DADHuQwBEkOASgwBKjEEqMQQoBJDgEoMASoxBKjqunucmVf/I3t35/QMd+z3OXb7nK/u1mUIkBgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUNbt7egaA41yGAIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQ1XX3ODOv/jxld+f0DHfs9zl2+5yv7tZlCJAYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVDW7e3oGgONchgCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUNV19zgzr/48ZXfn9Ax37Pc5dvucr+7WZQiQGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlQ1u3t6BoDjXIYAiSFAJYYAlRgCVGIIUIkhQFW/DzpQwzEfSLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_discriminator_loss: tf.Tensor(6.472708e-07, shape=(), dtype=float32)\n",
      "epoch: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAADnCAYAAACEyTRLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADcUlEQVR4nO3bsW3EQAwAwaWh/lumO1AmnCXPpJcQDBYE/jW7G8B/93N6AIC/QAwBEkOASgwBKjEEqOq6e5yZV//UvLtzeoY79vscu33OV3frMgRIDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKhqdvf0DADHuQwBEkOASgwBKjEEqMQQoBJDgEoMASoxBKjqunucmVf/I3t35/QMd+z3OXb7nK/u1mUIkBgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUNbt7egaA41yGAIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQ1XX3ODOv/jxld+f0DHfs9zl2+5yv7tZlCJAYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVDW7e3oGgONchgCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUNV19zgzr/48ZXfn9Ax37Pc5dvucr+7WZQiQGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlQ1u3t6BoDjXIYAiSFAJYYAlRgCVGIIUIkhQFW/DzpQwzEfSLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_discriminator_loss: tf.Tensor(2.4612882e-07, shape=(), dtype=float32)\n",
      "epoch: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAADnCAYAAACEyTRLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADcUlEQVR4nO3bsW3EQAwAwaWh/lumO1AmnCXPpJcQDBYE/jW7G8B/93N6AIC/QAwBEkOASgwBKjEEqOq6e5yZV//UvLtzeoY79vscu33OV3frMgRIDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKhqdvf0DADHuQwBEkOASgwBKjEEqMQQoBJDgEoMASoxBKjqunucmVf/I3t35/QMd+z3OXb7nK/u1mUIkBgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUNbt7egaA41yGAIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQ1XX3ODOv/jxld+f0DHfs9zl2+5yv7tZlCJAYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVDW7e3oGgONchgCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUNV19zgzr/48ZXfn9Ax37Pc5dvucr+7WZQiQGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlQ1u3t6BoDjXIYAiSFAJYYAlRgCVGIIUIkhQFW/DzpQwzEfSLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_discriminator_loss: tf.Tensor(9.2051937e-07, shape=(), dtype=float32)\n",
      "epoch: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAADnCAYAAACEyTRLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAADcUlEQVR4nO3bsW3EQAwAwaWh/lumO1AmnCXPpJcQDBYE/jW7G8B/93N6AIC/QAwBEkOASgwBKjEEqOq6e5yZV//UvLtzeoY79vscu33OV3frMgRIDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKjEEKASQ4BKDAEqMQSoxBCgEkOASgwBKjEEqMQQoBJDgEoMASoxBKhqdvf0DADHuQwBEkOASgwBKjEEqMQQoBJDgEoMASoxBKjqunucmVf/I3t35/QMd+z3OXb7nK/u1mUIkBgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUNbt7egaA41yGAIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQ1XX3ODOv/jxld+f0DHfs9zl2+5yv7tZlCJAYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVDW7e3oGgONchgCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUNV19zgzr/48ZXfn9Ax37Pc5dvucr+7WZQiQGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlRiCFCJIUAlhgCVGAJUYghQiSFAJYYAlRgCVGIIUIkhQCWGAJUYAlQ1u3t6BoDjXIYAiSFAJYYAlRgCVGIIUIkhQFW/DzpQwzEfSLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_discriminator_loss: tf.Tensor(2.0547718e-06, shape=(), dtype=float32)\n",
      "epoch: 8\n"
     ]
    }
   ],
   "source": [
    "len_train_set = train_mnist_image.shape[0]\n",
    "fake_label = np.zeros((batch_size, 1))\n",
    "real_label = np.ones((batch_size, 1))\n",
    "for i in range(EPOCHS):\n",
    "    image_batches = batch_generator(train_mnist_image , batch_size)\n",
    "    for real_image_batch in image_batches:\n",
    "        [generator_loss, discriminator_loss] = train_step(real_image_batch)\n",
    "    \n",
    "    predictions = generator(np.random.normal(0, 1, (batch_size, 10)), training=False)\n",
    "    for j in range(predictions[0:16].shape[0]):\n",
    "        plt.subplot(4, 4, j+1)\n",
    "        plt.imshow(predictions[j, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(i))\n",
    "    plt.show()\n",
    "    print(\"total_discriminator_loss: \" + str(discriminator_loss))\n",
    "    print(\"epoch: \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for image in images: # how many imgs will show from the 3x3 grid\n",
    "    fig, axs = plt.subplots(nrows=1, sharex=True, figsize=(2, 2))\n",
    "    two_d = (np.reshape(image, (28, 28)))\n",
    "    axs.imshow(two_d, interpolation='nearest', cmap=plt.get_cmap('gray'))\n",
    "    i=i+1\n",
    "    if i%100 == 0:\n",
    "        a=input()\n",
    "        if a=='q':\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 244412869876514494\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6910041152\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5429016145505624272\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 3070, pci bus id: 0000:23:00.0, compute capability: 8.6\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_available_gpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1bd066bf156a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_available_gpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_available_gpus' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:XLA_GPU:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
